@@ -254,7 +254,8 @@ for b in tfds.text.super_glue.SuperGlue.builder_configs.values():
       "super_glue_%s_v102" % b.name,
       source=seqio.TfdsDataSource(
           tfds_name="super_glue/%s:1.0.2" % b.name,
-          splits=["test"] if b.name in ["axb", "axg"] else None),
+          # splits=["test"] if b.name in ["axb", "axg"] else None),
+          splits={"train": "train", "validation": "validation", "test": "test"}),
       preprocessors=glue_preprocessors,
       metric_fns=get_super_glue_metric(b.name),
       output_features=DEFAULT_OUTPUT_FEATURES,
@@ -335,7 +336,8 @@ TaskRegistry.add(
 # Maximized evaluation metrics over all answers.
 TaskRegistry.add(
     "squad_v010_allanswers",
-    source=seqio.TfdsDataSource(tfds_name="squad/v1.1:3.0.0"),
+    source=seqio.TfdsDataSource(
+        tfds_name="squad/v1.1:3.0.0", splits={"test": "validation[:10%]"}),
     preprocessors=[
         preprocessors.squad,
         seqio.preprocessors.tokenize,
diff --git a/t5/evaluation/eval_utils.py b/t5/evaluation/eval_utils.py
index 9e7cf54..006264c 100644
--- a/t5/evaluation/eval_utils.py
+++ b/t5/evaluation/eval_utils.py
@@ -127,10 +127,12 @@ def get_eval_metric_values(events, task_name=None):
   eval_values = {}
   for tag, event_values in events.items():
     if tag.startswith("eval"):
-      if task_name:
+      if task_name.count("/") == 1:
         _, metric_name = tag.split("/")
-      else:
+      elif task_name.count("/") == 1:
         _, task_name_from_tag, metric_name = tag.split("/")
+      else:
+        raise ValueError("Something wrong with the eval and task name.")
       eval_task_name = task_name if task_name else task_name_from_tag
       eval_values["{}/{}".format(eval_task_name, metric_name)] = event_values
   return eval_values